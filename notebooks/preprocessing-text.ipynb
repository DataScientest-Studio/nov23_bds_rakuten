{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processing suite à détection langue et avant/après traduction?\n",
    "Accès au dossier du projet sur Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/', force_remount=True)\n",
    "#%cd drive/MyDrive/Projet_Rakuten\n",
    "%cd G:\\.shortcut-targets-by-id\\1xn44LDjqEgryLM_F1Lo5CuZZnrJGMnfi\\Projet_Rakuten\\Output\\preprocessing-lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from nltk.tokenize import PunktSentenceTokenizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from wordcloud import WordCloud #, STOPWORDS\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Télécharger les ressources de nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du fichier source \\data\\output\\preprocessing-lang\\X_train_pre.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('X_train_pre.csv', index_col=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrage stop Words et Tokenization du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Envi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une liste personnalisée de stopwords français, anglais et allemends\n",
    "french_stopwords = set(nltk_stopwords.words('french'))\n",
    "english_stopwords = set(nltk_stopwords.words('english'))\n",
    "dutch_stopwords = set(nltk_stopwords.words('dutch'))\n",
    "combined_stopwords = french_stopwords.union(english_stopwords)\n",
    "combined_stopwords = combined_stopwords.union(dutch_stopwords)\n",
    "\n",
    "# Fonction pour nettoyer et extraire les mots, en excluant les stopwords\n",
    "def extract_words(text):\n",
    "    # Utilisation d'une expression régulière pour ne conserver que les mots\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    # Filtrage des mots en excluant les stopwords, les chiffres et les mots de moins de 2 lettres, et suppression des doublons\n",
    "    unique_words = {word for word in words if word not in combined_stopwords and not word.isdigit() and len(word) > 1}\n",
    "    return list(unique_words)\n",
    "\n",
    "# Ajout colonne mots_text contenant les mots de la colonne texte\n",
    "df['mots_text'] = df['texte'].apply(extract_words)\n",
    "\n",
    "# Affichage du résultat\n",
    "display(df.head())\n",
    "display(df['texte'][0])\n",
    "df['mots_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour convertir les tags POS de nltk à ceux compatibles avec WordNetLemmatizer (merci chatGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui permet de déterminer s'il faut lemmatiser un verbe, un nom,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation du lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour lemmatiser une liste de mots\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application de la fonction de lemmatisation à la colonne 'mots_text'. ATTENTION CODE > 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mots_lemmatises'] = df['mots_text'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du DataFrame résultant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonne mots_lemmatises est d'abord convertie en une colonne texte_lemmatise où chaque liste de mots est transformée en une chaîne de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les listes de mots en chaînes de caractères\n",
    "df['texte_lemmatise'] = df['mots_lemmatises'].apply(lambda x: ' '.join(x))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppression des colonnes superflues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['texte', 'mots_text', 'texte', 'mots_lemmatises'],axis=1)\n",
    "X.rename(columns={'texte_lemmatise': 'texte'}, inplace=True)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export preprocessed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les lignes où la colonne 'texte' contient NaN\n",
    "lignes_avec_nan = X['texte'].isna()\n",
    "\n",
    "# Afficher les lignes avec NaN\n",
    "print(\"\\nLignes avec NaN dans la colonne 'texte':\")\n",
    "print(X.loc[lignes_avec_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"X_preprocess_to_trad.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
